{"cells":[{"cell_type":"markdown","metadata":{"id":"AoxOjIlOImwx"},"source":["# Getting Started\n","\n","This notebook is inspired to the Stable Baselines3 tutorial available at [https://github.com/araffin/rl-tutorial-jnrr19](https://github.com/araffin/rl-tutorial-jnrr19).\n","\n","\n","## Introduction\n","\n","In this notebook, we will learn how to use **Gymnasium** environments and the basics of **Stable Baselines3**: how to instance an RL algorithm, train and evaluate it.\n","\n","### Links\n","\n","Gymnasium Github: [https://github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium)\n","\n","Gymnasium Documentation: [https://gymnasium.farama.org/index.html](https://gymnasium.farama.org/index.html#)\n","\n","Stable Baselines 3 Github:[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n","\n","Stable Baseline 3 Documentation: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)\n","\n","## Install Gymnasium and Stable Baselines3 Using Pip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSw-cRdT2djP"},"outputs":[],"source":["!pip install gymnasium\n","!pip install renderlab  #For rendering\n","!pip install stable-baselines3[extra]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_mU9TNfvE3D"},"outputs":[],"source":["import gymnasium as gym\n","import renderlab\n","import stable_baselines3\n","\n","print(gym.__version__)\n","print(stable_baselines3.__version__)"]},{"cell_type":"markdown","metadata":{"id":"_qoYv8gHvE3F"},"source":["## Initializing Environments\n","\n","Initializing environments in Gym and is done as follows. We can find a list of available environment [here](https://gym.openai.com/envs/#classic_control)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTbSOamAvE3G"},"outputs":[],"source":["env = gym.make('CartPole-v1')\n","\n","env_eval = gym.make('CartPole-v1', render_mode = \"rgb_array\")\n","env_eval = renderlab.RenderFrame(env_eval, \"./output\")"]},{"cell_type":"markdown","metadata":{"id":"EUQj4xduvE3G"},"source":["\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n","\n","Cartpole Environment Decription: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n","\n","Cartpole Source Code: [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)\n","\n","![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)"]},{"cell_type":"markdown","metadata":{"id":"OCgyoKJwvE3H"},"source":["## Interacting with the Environment\n","\n","We run an instance of `CartPole-v1` environment for 30 timesteps, showing the information returned by the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BisGya56vE3H"},"outputs":[],"source":["state, _ = env.reset() # resets the environment in the initial state\n","print(\"Initial state: \", state)\n","\n","for _ in range(30):\n","    action = env.action_space.sample() # sample a random action\n","\n","    state, reward, terminated, truncated, _ = env.step(action)  # execute the action in the environment\n","    print(\"State:\", state,\n","          \"Action:\", action,\n","          \"Reward:\", reward,\n","          \"Terminated:\", terminated,\n","          \"Truncated:\", truncated)\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"7ikNk64jvE3H"},"source":["A Gymnasium environment provides to the user mainly four methods:\n","\n","* `reset()`: resets the environment to its initial state $S_0 \\sim d_0$ and returns the observation corresponding to the initial state.\n","\n","\n","* `step(action)`: takes an action $A_t$ as an input and executes the action in current state $S_t$ of the environment. This method returns a tuple of four values:\n","\n","    * `observation` (object): an environment-specific object representation of your observation of the environment after the action is executed. It corresponds to the observation of the next state $S_{t+1} \\sim p(\\cdot|S_t,A_t)$\n","    \n","    * `reward` (float): immediate reward $R_{t+1} = r(S_t,A_t)$ obtained by executing action $A_t$ in state $S_t$\n","    \n","    * `terminated`(boolean): whether the reached next state $S_{t+1}$ is a terminal state.\n","    \n","    * `truncated`(boolean): whether the trajectory has reached the maximum number of steps.**testo in grassetto**\n","    \n","    * `info` (dict): additional information useful for debugging and environment-specific.\n","    \n","    \n","*  `render(method='human')`: allows visualizing the agent in action. Note that graphical interface does not work on Google Colab, so we cannot use it directly (we will need a workaround).\n","\n","\n","*  `seed()`: sets the seed for this environment’s random number generator."]},{"cell_type":"markdown","metadata":{"id":"fYnUyQQDvE3H"},"source":["## Observation and Action Spaces\n","\n","*  `observation_space`: this attribute provides the format of valid observations $\\mathcal{S}$. It is of datatype `Space` provided by Gymnasium. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers.\n","\n","*  `action_space`: this attribute provides the format of valid actions $\\mathcal{A}$. It is of datatype `Space` provided by Gymnasium. For example, if the action space is of type `Discrete` and gives the value `Discrete(2)`, this means there are two valid discrete actions: 0 and 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3V7sIG6evE3I"},"outputs":[],"source":["print(env.observation_space)\n","\n","print(env.action_space)\n","\n","print(env.observation_space.high)\n","\n","print(env.observation_space.low)"]},{"cell_type":"markdown","metadata":{"id":"HtNdPfS-vE3I"},"source":["`Spaces` types available in Gymnasium:\n","\n","*  `Box`: an $n$-dimensional compact space (i.e., a compact subset of $\\mathbb{R}^n$). The bounds of the space are contained in the `high` and `low` attributes.\n","\n","\n","*  `Discrete`: a discrete space made of $n$ elements, where $\\{0,1,\\dots,n-1\\}$ are the possible values.\n","\n","\n","Other `Spaces` types can be used: `Dict`, `Tuple`, `MultiBinary`, `MultiDiscrete`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t15LJ3sxvE3I"},"outputs":[],"source":["import numpy as np\n","from gymnasium.spaces import Box, Discrete\n","\n","observation_space = Box(low=-1.0, high=2.0, shape=(3,), dtype=np.float32)\n","print(observation_space.sample())\n","\n","observation_space = Discrete(4)\n","print(observation_space.sample())"]},{"cell_type":"markdown","metadata":{"id":"kwoqXiGwvE3I"},"source":["## Details on the Cartpole Environment\n","\n","From [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)\n","\n","A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n","\n","### Action Space\n","The action space is `action` in $\\{0,1\\}$, where `action` is used to push the cart with a fixed amount of force:\n","\n"," | Num | Action                 |\n","    |-----|------------------------|\n","    | 0   | Push cart to the left  |\n","    | 1   | Push cart to the right |\n","    \n","Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it.\n","    \n","### Observation Space\n","The observation is a `ndarray` with shape `(4,)` where the elements correspond to the following:\n","\n","   | Num | Observation           | Min                  | Max                |\n","    |-----|-----------------------|----------------------|--------------------|\n","    | 0   | Cart Position         | -4.8*                | 4.8*                |\n","    | 1   | Cart Velocity         | -Inf                 | Inf                |\n","    | 2   | Pole Angle            | ~ -0.418 rad (-24°)**| ~ 0.418 rad (24°)** |\n","    | 3   | Pole Angular Velocity | -Inf                 | Inf                |\n","\n","**Note:** above denotes the ranges of possible observations for each element, but in two cases this range exceeds the range of possible values in an un-terminated episode:\n","- `*`: the cart x-position can be observed between `(-4.8, 4.8)`, but an episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n","- `**`: Similarly, the pole angle can be observed between  `(-.418, .418)` radians or precisely **±24°**, but an episode is  terminated if the pole angle is outside the `(-.2095, .2095)` range or precisely **±12°**\n","    \n","### Rewards\n","Reward is 1 for every step taken, including the termination step.\n","\n","### Starting State\n","All observations are assigned a uniform random value between (-0.05, 0.05)\n","\n","### Episode Termination\n","The episode terminates of one of the following occurs:\n","1. Pole Angle is more than ±12°\n","2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n","3. Episode length is greater than 500\n"]},{"cell_type":"markdown","metadata":{"id":"xKDySnoVvE3I"},"source":["## Evaluation of some Simple Policies\n","\n","We now evaluate some policies on the cartpole.\n","\n","* **Uniform Policy**: uniformly random policy\n","\n","$$\n","\\pi(a|s) = \\mathrm{Uni}(\\{0,1\\})\n","$$\n","\n","* **Reactive Policy**: simple deterministic policy that selects the action based on the pole angle\n","\n","$$\n","\\pi(s) = \\begin{cases}\n","                0 & \\text{if Pole Angle } \\le 0 \\\\\n","                1 & \\text{otherwise}\n","            \\end{cases}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eW_UAusJvE3I"},"outputs":[],"source":["class UniformPolicy:\n","\n","    def predict(self, obs):\n","        return np.random.randint(0, 2), obs  # return the observation to comply with stable-baselines3\n","\n","\n","class ReactivePolicy:\n","\n","    def predict(self, obs):\n","        if obs[2] <= 0:\n","            return 0, obs\n","        else:\n","            return 1, obs"]},{"cell_type":"markdown","metadata":{"id":"mPvHvRH3vE3J"},"source":["Let us create a function to evaluate the agent's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKi5PLUkvE3J"},"outputs":[],"source":["def evaluate(env, policy, gamma=1., num_episodes=100):\n","    \"\"\"\n","    Evaluate a RL agent\n","    :param env: (Env object) the Gym environment\n","    :param policy: (BasePolicy object) the policy in stable_baselines3\n","    :param gamma: (float) the discount factor\n","    :param num_episodes: (int) number of episodes to evaluate it\n","    :return: (float) Mean reward for the last num_episodes\n","    \"\"\"\n","    all_episode_rewards = []\n","    for i in range(num_episodes): # iterate over the episodes\n","        episode_rewards = []\n","        done = False\n","        discounter = 1.\n","        obs, _ = env.reset()\n","        while not done: # iterate over the steps until termination\n","            action, _ = policy.predict(obs)\n","            obs, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","            episode_rewards.append(reward * discounter) # compute discounted reward\n","            discounter *= gamma\n","\n","        all_episode_rewards.append(sum(episode_rewards))\n","\n","    mean_episode_reward = np.mean(all_episode_rewards)\n","    std_episode_reward = np.std(all_episode_rewards) / np.sqrt(num_episodes - 1)\n","    print(\"Mean reward:\", mean_episode_reward,\n","          \"Std reward:\", std_episode_reward,\n","          \"Num episodes:\", num_episodes)\n","\n","    return mean_episode_reward, std_episode_reward"]},{"cell_type":"markdown","metadata":{"id":"afBSiT_tvE3J"},"source":["Let us test the uniform policy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g33tYX96vE3J"},"outputs":[],"source":["uniform_policy = UniformPolicy()\n","\n","uniform_policy_mean, uniform_policy_std = evaluate(env, uniform_policy)\n","\n","_, _ = evaluate(env_eval, uniform_policy, num_episodes=1)\n","env_eval.play()"]},{"cell_type":"markdown","metadata":{"id":"cf3hyxHavE3K"},"source":["Let us test the reactive policy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpsJqDB2vE3K"},"outputs":[],"source":["reactive_policy = ReactivePolicy()\n","\n","reactive_policy_mean, reactive_policy_std = evaluate(env, reactive_policy)\n","\n","_, _ = evaluate(env_eval, reactive_policy, num_episodes=1)\n","env_eval.play()"]},{"cell_type":"markdown","metadata":{"id":"lhze0bb_vE3K"},"source":["## PPO Training\n","\n","We now use Stable Baselines3 to train some simple algorithms. We start by using [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html).\n","\n","We select the [MlpPolicy](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#ppo-policies) because the state of the CartPole environment is a feature vector (not images for instance). The type of action to use (discrete/continuous) will be automatically deduced from the environment action space.\n","\n","We consider two network architectures:\n","\n","* Linear policy\n","* Two hidden layers of 32 neurons each"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-NBhhS_vE3K","scrolled":false},"outputs":[],"source":["from stable_baselines3 import PPO\n","\n","# Instantiate the algorithm with 32x32 NN approximator for both actor and critic\n","ppo_mlp = PPO(\"MlpPolicy\", env, verbose=1,\n","                learning_rate=0.01,\n","                policy_kwargs=dict(net_arch = [dict(pi=[32, 32], vf=[32, 32])]))\n","\n","print(ppo_mlp.policy)\n","\n","# Instantiate the algorithm with linear approximator for both actor and critic\n","ppo_linear = PPO(\"MlpPolicy\", env, verbose=1,\n","                   learning_rate=0.01,\n","                   policy_kwargs=dict(net_arch = [dict(pi=[], vf=[])]))\n","\n","print(ppo_linear.policy)"]},{"cell_type":"markdown","metadata":{"id":"gMsk-dD0vE3K"},"source":["Let us now train the algorithms. In order to keep track of the performance during learning, we can log the evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnpEj4BWvE3L"},"outputs":[],"source":["# Train the agent for 50000 steps\n","ppo_mlp.learn(total_timesteps=50000, log_interval=4, progress_bar=True)\n","\n","ppo_linear.learn(total_timesteps=50000, log_interval=4, progress_bar=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLDoA0CpvE3M"},"outputs":[],"source":["# Evaluate the trained models\n","ppo_mlp_mean, ppo_mlp_std = evaluate(env, ppo_mlp)\n","\n","_, _ = evaluate(env_eval, ppo_mlp, num_episodes=1)\n","env_eval.play()\n","\n","\n","ppo_linear_mean, ppo_linear_std = evaluate(env, ppo_linear)\n","\n","_, _ = evaluate(env_eval, ppo_linear, num_episodes=1)\n","env_eval.play()"]},{"cell_type":"markdown","metadata":{"id":"_gmOZ_6DvE3M"},"source":["Let us have a look at the weights learned by PPO with the linear policy. Since actions are discrete, the policy model is **softmax**:\n","\n","$$\n","\\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s}) \\propto \\exp \\left( \\mathbf{s}^T \\boldsymbol{\\theta}(a) + b(a) \\right)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7pFa60tvE3M"},"outputs":[],"source":["print(ppo_linear.policy.action_net.weight)\n","print(ppo_linear.policy.action_net.bias)"]},{"cell_type":"markdown","metadata":{"id":"cKNVraYivE3N"},"source":["## DQN Training"]},{"cell_type":"markdown","metadata":{"id":"sg9-M4rAvE3N"},"source":["Let us now try [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with an MlpPolicy as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gU0fNecsvE3N"},"outputs":[],"source":["from stable_baselines3 import DQN\n","from torch import nn\n","\n","\n","# Instantiate the algorithm with 32x32 NN approximator\n","dqn_mlp = DQN(\"MlpPolicy\", env, verbose=1,\n","                learning_starts=3000,\n","                policy_kwargs=dict(net_arch = [32, 32], activation_fn=nn.Tanh))\n","\n","print(dqn_mlp.policy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ye-yH_qsvE3N"},"outputs":[],"source":["# Train the agent for 50000 steps\n","dqn_mlp.learn(total_timesteps=50000, log_interval=100, progress_bar=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uowI-OkYvE3S"},"outputs":[],"source":["# Evaluate the trained models\n","dqn_mlp_mean, dqn_mlp_std = evaluate(env, dqn_mlp)\n","\n","_, _ = evaluate(env_eval, dqn_mlp, num_episodes=1)\n","env_eval.play()"]},{"cell_type":"markdown","metadata":{"id":"ljAhob-XvE3S"},"source":["Let us now plot the final results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBNTTXoavE3S"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","#Plot the results\n","fig = plt.figure()\n","ax = fig.add_axes([0,0,1,1])\n","\n","algs = ['Random', 'Reactive', 'PPO MLP', 'PPO Linear', 'DQN']\n","means = [uniform_policy_mean, reactive_policy_mean, ppo_mlp_mean, ppo_linear_mean, dqn_mlp_mean]\n","errors = [uniform_policy_std, reactive_policy_std, ppo_mlp_std, ppo_linear_std, dqn_mlp_std]\n","\n","ax.bar(algs, means, yerr=errors, align='center', alpha=0.5, ecolor='black', capsize=10)\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/albertometelli/rl-phd-2022/blob/main/01_getting_started.ipynb","timestamp":1707582384889}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}