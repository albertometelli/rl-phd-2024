{"cells":[{"cell_type":"markdown","metadata":{"id":"AoxOjIlOImwx"},"source":["# G(PO)MDP\n","\n","This notebook is inspired to the Stable Baselines3 tutorial available at [https://github.com/araffin/rl-tutorial-jnrr19](https://github.com/araffin/rl-tutorial-jnrr19).\n","\n","\n","## Introduction\n","\n","In this notebook, we will learn how to build a customized environment with **Gymnasium**.\n","\n","### Links\n","\n","Gymnasium Github: [https://github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium)\n","\n","Gymnasium Documentation: [https://gymnasium.farama.org/index.html](https://gymnasium.farama.org/index.html#)\n","\n","Stable Baselines 3 Github:[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n","\n","Stable Baseline 3 Documentation: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)\n","\n","## Install Gymnasium and Stable Baselines3 Using Pip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp8rSS4DIhEV"},"outputs":[],"source":["!pip install gymnasium\n","!pip install renderlab  #For rendering\n","!pip install stable-baselines3[extra]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hH_BePqY-wkD"},"outputs":[],"source":["import gymnasium as gym\n","import renderlab\n","import stable_baselines3\n","\n","print(gym.__version__)\n","print(stable_baselines3.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixSc2pjd-wkF"},"outputs":[],"source":["def evaluate(env, policy, gamma=1., num_episodes=100):\n","    \"\"\"\n","    Evaluate a RL agent\n","    :param env: (Env object) the Gym environment\n","    :param policy: (BasePolicy object) the policy in stable_baselines3\n","    :param gamma: (float) the discount factor\n","    :param num_episodes: (int) number of episodes to evaluate it\n","    :return: (float) Mean reward for the last num_episodes\n","    \"\"\"\n","    all_episode_rewards = []\n","    for i in range(num_episodes): # iterate over the episodes\n","        episode_rewards = []\n","        done = False\n","        discounter = 1.\n","        obs, _ = env.reset()\n","        while not done: # iterate over the steps until termination\n","            action, _ = policy.predict(obs)\n","            obs, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","            episode_rewards.append(reward * discounter) # compute discounted reward\n","            discounter *= gamma\n","\n","        all_episode_rewards.append(sum(episode_rewards))\n","\n","    mean_episode_reward = np.mean(all_episode_rewards)\n","    std_episode_reward = np.std(all_episode_rewards) / np.sqrt(num_episodes - 1)\n","    print(\"Mean reward:\", mean_episode_reward,\n","          \"Std reward:\", std_episode_reward,\n","          \"Num episodes:\", num_episodes)\n","\n","    return mean_episode_reward, std_episode_reward"]},{"cell_type":"markdown","metadata":{"id":"INYVssHb-wkG"},"source":["## Plotting\n","\n","A helper function to plot the learning curves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlECSDkJ-wkG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","def plot_results(results):\n","    plt.figure()\n","\n","    _mean = []\n","    _std = []\n","    for m, s in results:\n","        _mean.append(m)\n","        _std.append(s)\n","\n","    _mean = np.array(_mean)\n","    _std = np.array(_std)\n","\n","    ts = np.arange(len(_mean))\n","    plt.plot(ts, _mean, label='G(PO)MDP')\n","    plt.fill_between(ts, _mean-_std, _mean+_std, alpha=.2)\n","\n","    plt.xlabel('Trajectories')\n","    plt.ylabel('Average return')\n","    plt.legend(loc='lower right')\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"f5LmgBQP-wkH"},"source":["## G(PO)MDP\n","\n","![ss](gpomdp.png)\n","\n","**References**\n","\n","Baxter, Jonathan, and Peter L. Bartlett. \"Infinite-horizon policy-gradient estimation.\" Journal of Artificial Intelligence Research 15 (2001): 319-350."]},{"cell_type":"markdown","metadata":{"id":"lyQq6UjY-wkI"},"source":["## Policy\n","\n","We will use a Gaussian policy, linear in the state variables and with fixed (non-learnable) standard deviation.\n","\n","$$\n","\\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s}) = \\mathcal{N}(a| \\boldsymbol{\\theta}^T \\mathbf{s}, \\sigma^2)\n","$$\n","\n","The policy must implement the usual `predict` method and some additional methods for computing the policy gradient. Specifically, we will need a `grad_log` method to return the gradient of the logarithm of the policy (the score):\n","\n","$$\n","\\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s})= \\frac{(a - \\boldsymbol{\\theta}^T \\mathbf{s})\\mathbf{s}}{\\sigma^2}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Yl621Jxc-wkJ"},"source":["## Exercise 1\n","\n","Complete the implementation of the methods `predict` and `grad_log`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpgkHHxF-wkJ"},"outputs":[],"source":["class GaussianPolicy:\n","\n","    def __init__(self, dim, std=0.1):\n","        \"\"\"\n","        :param dim: number of state variables\n","        :param std: fixed standard deviation\n","        \"\"\"\n","\n","        self.std = std\n","        self.dim = dim\n","        self.theta = np.zeros((dim,))  # zero initializatoin\n","\n","    def get_theta(self):\n","        return self.theta\n","\n","    def set_theta(self, value):\n","        self.theta = value\n","\n","    def predict(self, obs):\n","        \"\"\"\n","        :param obs: (ndarray) the state observation (dim,)\n","        :return: the sampled action and the same observation\n","        \"\"\"\n","        action = 0.\n","\n","        #TODO\n","\n","        return np.array([action]), obs\n","\n","    def grad_log(self, obs, action):\n","        \"\"\"\n","        :param obs: (ndarray) the state observation (dim,)\n","        :param action: (float) the action\n","        :return: (ndarray) the score of the policy (dim,)\n","        \"\"\"\n","        grad_log = 0.\n","\n","        #TODO\n","\n","        return grad_log"]},{"cell_type":"markdown","metadata":{"id":"YfOHjWrY-wkK"},"source":["## Exercise 1 - Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoaDJuBC-wkK"},"outputs":[],"source":["class GaussianPolicy:\n","\n","    def __init__(self, dim, std=0.1):\n","        \"\"\"\n","        :param dim: number of state variables\n","        :param std: fixed standard deviation\n","        \"\"\"\n","\n","        self.std = std\n","        self.dim = dim\n","        self.theta = np.zeros((dim,))  # zero initializatoin\n","\n","    def get_theta(self):\n","        return self.theta\n","\n","    def set_theta(self, value):\n","        self.theta = value\n","\n","    def predict(self, obs):\n","        mean = np.dot(obs, self.theta)\n","        action = mean + np.random.randn() * self.std\n","        return np.array([action]), obs\n","\n","    def grad_log(self, obs, action):\n","        mean = np.dot(obs, self.theta)\n","        grad_log = (action - mean) * obs / self.std ** 2\n","        return grad_log"]},{"cell_type":"markdown","metadata":{"id":"P5QDWSfP-wkL"},"source":["## Training Routine\n","\n","We provide the already implemented skeleton of the training routine that samples at every iterations $m$ trajectories from the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwH2vqlq-wkO"},"outputs":[],"source":["def collect_rollouts(env, policy, m, T):\n","    \"\"\"\n","    Collects m rollouts by running the policy in the\n","        environment\n","    :param env: (Env object) the Gym environment\n","    :param policy: (Policy object) the policy\n","    :param gamma: (float) the discount factor\n","    :param m: (int) number of episodes per iterations\n","    :param K: (int) maximum number of iterations\n","    :param theta0: (ndarray) initial parameters (d,)\n","    :param alpha: (float) the constant learning rate\n","    :param T: (int) the trajectory horizon\n","    :return: (list of lists) one list per episode\n","                each containing triples (s, a, r)\n","    \"\"\"\n","\n","    ll = []\n","    for j in range(m):\n","        s, _ = env.reset()\n","        t = 0\n","        done = False\n","        l = []\n","        while t < T and not done:\n","            a, _ = policy.predict(s)\n","            s1, r, done, _, _ = env.step(a)\n","            l.append((s, a, r))\n","            s = s1\n","            t += 1\n","        ll.append(l)\n","    return ll\n","\n","def train(env, policy, gamma, m, K, alpha, T):\n","    \"\"\"\n","    Train a policy with G(PO)MDP\n","    :param env: (Env object) the Gym environment\n","    :param policy: (Policy object) the policy\n","    :param gamma: (float) the discount factor\n","    :param m: (int) number of episodes per iterations\n","    :param K: (int) maximum number of iterations\n","    :param alpha: (float) the constant learning rate\n","    :param T: (int) the trajectory horizon\n","    :return: list (ndarray, ndarray) the evaluations\n","    \"\"\"\n","\n","    results = []\n","\n","    # Evaluate the initial policy\n","    res = evaluate(env, policy, gamma)\n","    results.append(res)\n","\n","    for k in range(K):\n","\n","        print('Iteration:', k)\n","\n","        # Generate rollouts\n","        rollouts = collect_rollouts(env, policy, m, T)\n","\n","        # Get policy parameter\n","        theta = policy.get_theta()\n","\n","        # Call your G(PO)MDP estimator\n","        pg = gpomdp(rollouts, policy, gamma)\n","\n","        # Update policy parameter\n","        theta = theta + alpha * pg\n","\n","        # Set policy parameters\n","        policy.set_theta(theta)\n","\n","        # Evaluate the updated policy\n","        res = evaluate(env, policy, gamma)\n","        results.append(res)\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"eCer8hnx-wkS"},"source":["## Exercise 2\n","\n","Complete the following function `gpomdp` that computes the G(PO)MDP gradient estimator given rollout trajectories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zl_-esa5-wkS"},"outputs":[],"source":["def gpomdp(rollouts, policy, gamma):\n","    \"\"\"\n","    :param rollouts: (list of lists) generated by 'collect_rollouts'\n","    :param policy: (Policy object) the policy\n","    :param gamma: (float) the discount factor\n","    :return: (ndarray) the policy gradient (dim,)\n","    \"\"\"\n","\n","    grad = 0\n","\n","    #TODO\n","\n","    return grad"]},{"cell_type":"markdown","metadata":{"id":"1sJ2_i1J-wkS"},"source":["## Exercise 2 - Solution"]},{"cell_type":"markdown","metadata":{"id":"AVaGHSXX-wkT"},"source":["The following is a possible implementation of G(PO)MDP that turns out to be very inefficient! It can be improved by precomputing the scores at the beginning and using matrix opertions instead of for cicles."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGYZ4GZl-wkT"},"outputs":[],"source":["def gpomdp_inefficient(rollouts, policy, gamma):\n","\n","    grad = 0\n","\n","    # Very very inefficient implementation!\n","    for roll in rollouts:\n","        H = len(roll)\n","\n","        sum_rew = 0.\n","        for t in range(H):\n","\n","            sum_scores = 0.\n","            for l in range(t + 1):\n","                s, a, _ = roll[l]\n","                score = policy.grad_log(s, a)\n","                sum_scores += score\n","\n","            _, _, r = roll[t]\n","            sum_rew += gamma ** t * r * sum_scores\n","\n","        grad += sum_rew\n","\n","    return grad / len(rollouts)"]},{"cell_type":"markdown","metadata":{"id":"ULwImUbb-wkT"},"source":["We can speedup the implementation, by precomputing the cumulative sum of the scores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gxj0CdDr-wkU"},"outputs":[],"source":["def gpomdp(rollouts, policy, gamma):\n","\n","    grad = 0\n","\n","    # A little more efficient implementation!\n","    for roll in rollouts:\n","        H = len(roll)\n","        disc_rew = np.zeros((H, 1))\n","        scores = np.zeros((H, policy.dim))\n","\n","        for t in range(H):\n","            s, a, r = roll[t]\n","            disc_rew[t] = gamma ** t * r\n","            scores[t] = policy.grad_log(s, a)\n","\n","        cum_scores = np.cumsum(scores, axis=0)\n","\n","        grad += np.sum(cum_scores * disc_rew, axis=0)\n","\n","\n","    return grad / len(rollouts)"]},{"cell_type":"markdown","metadata":{"id":"6kBPuatb-wkU"},"source":["## Test our Implementation\n","\n","We test our G(PO)MDP implementation over the `MountainCarContinuous-v0` environment.\n","\n","MountainCarContinuous Environment Decription: [https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/)\n","\n","MountainCarContinuous Source Code: [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/continuous_mountain_car.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/continuous_mountain_car.py)\n","\n","We consider a modified simpler version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWUCb8S3-wka"},"outputs":[],"source":["import gymnasium as gym\n","from gymnasium.envs.classic_control.continuous_mountain_car import Continuous_MountainCarEnv\n","from gymnasium.envs.registration import register\n","from typing import Optional\n","\n","\n","class SimplifiedContinuous_MountainCarEnv(Continuous_MountainCarEnv):\n","\n","    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):\n","        super(SimplifiedContinuous_MountainCarEnv, self).__init__(render_mode, goal_velocity)\n","\n","        # We make the environment a little bit simpler by increasing the power\n","        self.power =  0.02\n","\n","\n","register(\n","    id=\"SimplifiedMountainCarContinuous-v1\",\n","    entry_point=\"__main__:SimplifiedContinuous_MountainCarEnv\",\n","    max_episode_steps=200,\n","    reward_threshold=100,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcsmR6RS-wkb"},"outputs":[],"source":["import numpy as np\n","\n","\n","# Instantiate the environment\n","env = gym.make('SimplifiedMountainCarContinuous-v1')\n","\n","env_eval = gym.make('SimplifiedMountainCarContinuous-v1', render_mode = \"rgb_array\")\n","env_eval = renderlab.RenderFrame(env_eval, \"./output\")\n","\n","# Instantiate the policy\n","policy = GaussianPolicy(env.observation_space.shape[0], std=0.2)\n","\n","gamma = 0.999  # discount factor\n","m = 100        # number of trajectories per iteration\n","K = 100        # maximum number of iterations\n","alpha = 0.001  # learning rate\n","T = 200        # lenght of each trajectory\n","\n","# Start training\n","results = train(env, policy, gamma, m, K, alpha, T)"]},{"cell_type":"markdown","metadata":{"id":"3DyBnMpz-wkb"},"source":["Let us render the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TaoJLuv-wkb"},"outputs":[],"source":["perf_mean, perf_std = evaluate(env, policy)\n","\n","evaluate(env_eval, policy, num_episodes=1)\n","env_eval.play()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6CiEGFI-wkb"},"outputs":[],"source":["plot_results(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pfm3X2Bq-wkc"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}